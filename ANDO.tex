\documentclass[a4paper, 12pt]{article}

% === Packages ===
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage[french]{babel}
\usepackage{libertine}
\usepackage[pdftex]{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage[OT1]{fontenc}
\usepackage[sfdefault,light,condensed]{roboto} % Font
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{array,multirow,makecell}
\usepackage{datetime}
\usepackage{xcolor}
\usepackage{lastpage}
\usepackage{float}
\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathabx}

\usepackage{tabularx}

\usepackage{url}
% === End Packages ===

\geometry{a4paper}
%\geometry{top=2cm, bottom=2cm, left=2cm, right=2cm}

\renewcommand{\contentsname}{Sommaire}
\renewcommand{\baselinestretch}{1}
%\renewcommand{\thepage}{}

% === Variables  ===
\newcommand{\nomcours}{ANDO - Analyse de données}
\newcommand{\nomprofesseur}{Regragui Mohamed}
\newcommand{\nomauteur}{Sébastien Goubeau}
% === End Variables ===

% === Commandes ===
\newtcolorbox{note}[1]{colback=green!5!white,colframe=green!65!black,fonttitle=\bfseries,title=#1}
\newtcolorbox{definition}[1]{colback=white,colframe=gray,fonttitle=\bfseries,title=#1}
\newtcolorbox{remarque}[1]{colback=green!5!white,colframe=green!65!black,fonttitle=\bfseries,title=#1}
\newtcolorbox{info}[1]{colback=white,colframe=blue!60!white,fonttitle=\bfseries,title=#1}
\newtcolorbox{rappel}[1]{colback=white,colframe=red!60!white,fonttitle=\bfseries,title=#1}
\newtcolorbox{theoreme}[1]{colback=white,colframe=red!90!white,fonttitle=\bfseries,title=#1}
\newtcolorbox{propriete}[1]{colback=white,colframe=orange!70!white,fonttitle=\bfseries,title=#1}
\newtcolorbox{proposition}[1]{colback=white,colframe=orange!50!white,fonttitle=\bfseries,title=#1}
% === End Commandes ===

\pagestyle{fancy}
\fancyhf{}
\lhead{\nomcours}
\rhead{\nomprofesseur}
\rfoot{Page \thepage{} / \pageref{LastPage}}
\cfoot{\date{\today}}

\begin{document}

\title{
\vspace{130pt}
\begin{centering}
\bigskip
\bigskip
\Huge {\textbf{\nomcours\\}}
\bigskip
\large{\textbf{Professeur} : \nomprofesseur \\ \textbf{Cours de} : \nomauteur}
\date{\today}
\end{centering}
}
\maketitle
\thispagestyle{empty}
\clearpage

\tableofcontents
\clearpage

\section{Description bidimensionnelle et mesure de corrélation}
\subsection{Loi conjointe de $\mathcal{X, Y}$}
Soient $\mathcal{X}$ et $\mathcal{Y}$ deux variables aléatoires discrètes définis sur le même espace probabilisé : ($\Omega{}, \varphi{}, P$).\\
Avec :
\begin{itemize}
\item $\Omega$ : Univers
\item $\varphi$ : Tribu (classe des évènements)
\item $P$ : Loi de définition restreinte
\end{itemize}

\begin{flushleft}
$\mathcal{X}(\Omega) = \{ x_i / i \in \mathcal{I} \}$ valeurs de $\mathcal{X}$\\
$\mathcal{Y}(\Omega) = \{ y_j / j \in \mathcal{J} \}$ valeurs de $\mathcal{Y}$
\end{flushleft}

\begin{definition}{Définition - Loi conjointe}
On appelle loi conjointe du couple ($\mathcal{X, Y}$) l'ensemble des couples ($x_i, y_j, P_{ij}$) où $x_i \in \mathcal{X}(\Omega), y_i \in \mathcal{Y}(\Omega)$\\
$P_{ij} = P ( (\mathcal{X} = x_i) \cap (\mathcal{Y} = y_j) )$
\end{definition}

\begin{remarque}{Remarque}
Si $\mathcal{I} = \ldbrack 1, r \rdbrack $ et $\mathcal{J} = \ldbrack 1, s \rdbrack$
{\Large
\begin{center}
\begin{tabular}{|c|ccccc|}
	\hline
	$\mathcal{X/Y}$ & $y_1$ & ...  & $y_j$ & ... & $y_s$ \\
	\hline
	$x_1$ &  &  & : &  & \\
	... &  &  & : &  & \\
	$x_i$ & ... & ... & $P_{ij}$ & ... & ...\\
	... &  &  & : &  & \\
	$x_r$ &  &  & : &  & \\
	\hline
\end{tabular}\\
\end{center}
}
$P_{ij} = P ( (\mathcal{X} = x_i) \cap (\mathcal{Y} = y_j) )$\\
$P_{ij} \geq 0$\\
$\sum_{i, j} P_{ij} = 1$\\
\end{remarque}


\subsection{Loi marginale}
\begin{definition}{Définition - Loi marginale}
Les variables $\mathcal{X}$ et $\mathcal{Y}$ sont appelé variables marginales.\\
$P (X = x_i) = \sum_{j \in \mathcal{J}} P_{ij} = \sum_{j \in \mathcal{J}} P ( (\mathcal{X} = x_i) \cap (\mathcal{Y} = y_j) )$\\
$P_{\forall j} (\mathcal{X} = x_i) = \sum_{j \in \mathcal{J}} P_{ij} = P_{i \bullet}$ (Notation statistique de la loi marginale)\\
$P_{\forall i} (\mathcal{Y} = y_j) = \sum_{i \in \mathcal{I}} P_{ij} = P_{\bullet j}$ (Notation statistique de la loi marginale)\\
\end{definition}

\subsubsection*{Exemple}
{\Large
\begin{center}
\begin{tabular}{|c|cccc|c|}
	\hline
	$\mathcal{X/Y}$ & 1 & 2  & 3 & 4 & $P_{i\bullet}$ \\
	\hline
	1 & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{4}$ \\
	2 & 0 & $\frac{2}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{4}$ \\
	3 & 0 & 0 & $\frac{3}{16}$ & $\frac{1}{16}$ & $\frac{1}{4}$ \\
	4 & 0 & 0 & 0 & $\frac{4}{16}$ & $\frac{1}{4}$ \\
	\hline
	$P_{\bullet j}$ & $\frac{1}{16}$ & $\frac{3}{16}$ & $\frac{5}{16}$ & $\frac{7}{16}$ & 1 \\
	\hline
\end{tabular}\\
\end{center}
}
\begin{flushleft}
$P (\mathcal{X} = 4) = \frac{1}{4}$\\
$P (\mathcal{Y} = 3) = \frac{5}{16}$\\
\end{flushleft}

Le 1 en bas à droite du tableau permet de vérifier que les calcules sont bon (la valeur doit toujours être à 1).


\subsection{Loi conditionnelle}
\begin{definition}{Définition - Loi Conditionnelle}
On appelle loi conditionnelle de $\mathcal{X} = x_i$ sachant $\mathcal{Y} = y_j$ : \\
$P (\mathcal{X} = x_i / \mathcal{Y} = y_i) = \frac{P ((\mathcal{X} = x_i) \cap (\mathcal{Y} = y_j))}{P (\mathcal{Y} = y_j)} = \frac{P_{ij}}{P_{\bullet j}}$ \\
$P (\mathcal{Y} = y_j / \mathcal{X} = x_i) = \frac{P_{ij}}{P_{i \bullet}}$ \\
\end{definition}


\subsubsection*{Exemple}
On reprend le tableau de l'exemple précédent.\\
$P (\mathcal{X} = 1 / \mathcal{Y} = 3) = \frac{P((\mathcal{X} = 1) \cap (\mathcal{Y} = 3)}{P (\mathcal{Y} = 3)} = \frac{\frac{1}{16}}{\frac{5}{16}} = \frac{1}{5}$\\

On peut s'aider d'un tableau :

{\Large
\begin{center}
\begin{tabular}{c|cccc}
	$x_i$ & 1 & 2  & 3 & 4 \\
	\hline
	$P (\mathcal{X} / \mathcal{Y} = 3)$ & $\frac{1}{5}$ & $\frac{1}{5}$ & $\frac{3}{5}$ & 0
\end{tabular}\\
\end{center}
}

\begin{definition}{Définition - Indépendance}
$\mathcal{X}$ et $\mathcal{Y}$ sont indépendant si et seulement si :\\
$P ((\mathcal{X}) = x) \cap (\mathcal{Y} = y)) = P (\mathcal{X}) = x) P(\mathcal{Y} = y))$\\
$\Leftrightarrow P_{ij} = P_{i \bullet} \times P_{\bullet j}$ ($\forall (ij) \in \mathcal{I} \times \mathcal{J}$)\\
$\Leftrightarrow P (\mathcal{X} = x / \mathcal{Y} = y) = P (\mathcal{X} = x)$
\end{definition}



\subsection{Loi d'une fonction de 2 variables ($\mathcal{Z} = g(\mathcal{X}, \mathcal{Y})$) (Loi composé)}
Soit $g: \mathbb{R}^2 \longrightarrow \mathbb{R}$ définit sur l'ensemble des valeurs prises par $\mathcal{X}$ et $\mathcal{Y}$.\\
$\mathcal{Z} = g(\mathcal{X}, \mathcal{Y})$\\
\begin{center}
$(\mathcal{Z} = zk) = \underset{(i, j) | g (x_i, y_i) = zk}{\cup} ((\mathcal{X} = x_i) \cap (\mathcal{Y} = y_i))$
\end{center}

\begin{remarque}{Remarque}
L'expression ci-dessus signifie en français :\\
\emph{L'événement $(\mathcal{Z} = zk)$ (symbolisé par des parenthèses) est égale à l'union des $(\mathcal{X} = x_i) \cap (\mathcal{Y} = y_i)$ qui vérifie le couple $(i, j)$ tel que $g (x_i, y_i) = zk$}.
\end{remarque}

\begin{center}
$\boxed{P (\mathcal{Z} = zk) = \underset{(i, j) | g (x_i, y_i) = zk}{\sum} P ((\mathcal{X} = x_i) \cap (\mathcal{Y} = y_i))}$
\end{center}

\begin{flushleft}
En particulier $g: (\mathcal{X, Y}) = \mathcal{X} + \mathcal{Y} = \mathcal{S}$\\
$P (\mathcal{S} = s) = \underset{(i, j) | x_i + y_j = s_k}{\sum} P ((\mathcal{X} = x_i) \cap (\mathcal{Y} = y_j))$
\end{flushleft}



\subsubsection*{Exemple}
Reprise de l'exemple précédent.\\
$\mathcal{S = X + Y}$\\
$\mathbb{P} = \mathcal{X . Y}$\\
Loi de $\mathcal{S}$

{\Large
\begin{center}
\begin{tabular}{c|ccccccc}
	$sk$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
	\hline
	$P (\mathcal{S} = sk)$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{3}{16}$ & $\frac{2}{16}$ &  $\frac{4}{16}$ & $\frac{1}{16}$ & $\frac{4}{16}$ \\
\end{tabular}\\
\end{center}
}

Pour remplire le tableau : $P (\mathcal{S} = 5) = P_{1, 4} + P_{2, 3} + P_{3, 2} + P_{4, 1} = \frac{2}{16} = \frac{1}{8}$

Loi du produit $\mathbb{P} = \mathcal{X . Y}$

{\Large
\begin{center}
\begin{tabular}{c|ccccccccc}
	$\mathbb{P}_i$ & 1 & 2 & 3 & 4 & 6 & 8 & 9 & 12 & 16 \\
	\hline
	$P (\mathbb{P} = \mathbb{P}_i)$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{3}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{3}{16}$ & $\frac{1}{16}$ & $\frac{4}{16}$ \\
\end{tabular}\\
\end{center}
}

La ligne doit être égale à 1.



\subsection{Espérance de $\mathcal{Z} = g(\mathcal{X, Y})$}
\begin{definition}{Définition - Espérance d'une Loi composé}
Soit $\mathcal{Z}$ une loi composé, l'espérance de cette loi est :
\begin{center}
$\boxed{E(\mathcal{Z}) = \underset{i, j}{\sum} g(x_i, y_j) P_{ij}}$
\end{center}
\end{definition}

\begin{remarque}{Remarque}
Si $\mathcal{X}$ et $\mathcal{Y}$ sont 2 variables indépendantes alors : $E(\mathcal{X, Y}) = E(\mathcal{X}) E(\mathcal{Y})$
\end{remarque}



\subsubsection*{Exemple}
{\Large
\begin{center}
\begin{tabular}{c|ccc|c}
	$\mathcal{X/Y}$ & 0 & 1 & 2 & $P_{i \bullet}$ \\
	\hline
	0 & $\frac{1}{20}$ \tiny{[0]} & $\frac{1}{4}$ \tiny{[0]} & 0 \tiny{[0]} & $\frac{3}{10}$ \tiny{[0]} \\
	1 & $\frac{17}{16}$ \tiny{[0]} & $\frac{1}{4}$ \tiny{[1]} & $\frac{1}{6}$ \tiny{[2]} & $\frac{7}{10}$ \tiny{[1]} \\
	\hline
	$P_{\bullet j}$ & $\frac{1}{3}$ \tiny{[0]} & $\frac{1}{2}$ \tiny{[1]} & $\frac{1}{6}$ \tiny{[2]} & 1\\
\end{tabular}\\
\end{center}
}

\begin{info}{Information}
Les indices entre crochet ("[0]") dans le tableau représente $i \times j$ (le nombre de la colonne fois celui de la ligne).\\
\textbf{Exemple :}
\begin{itemize}
\item colonne 0 ligne 0 (case $\frac{1}{20}$) : $0 \times 0 = 0$
\item colonne 1 ligne 2 (case $\frac{1}{6}$) : $1 \times 2 = 2$
\end{itemize}
\end{info}

\begin{flushleft}
\begin{tabular}{l|l}
$
\begin{aligned}
E (\mathcal{X . Y}) &= \overset{1}{\underset{i = 0}{\sum}} \overset{2}{\underset{j = 0}{\sum}} = i \times j \times P_{ij} \\
&= \frac{1}{4} \times 1 + \frac{1}{6} \times 2 \\
&\boxed{= \frac{7}{12}}
\end{aligned}
$ &
$
\begin{aligned}
E (\mathcal{X}) &= \frac{7}{10} \times 1 \\
&\boxed{= \frac{7}{10}}
\end{aligned}
$ \\
&
$
\begin{aligned}
E (\mathcal{Y}) &= \frac{1}{2} \times 1 + \frac{1}{6} \times 2
&\boxed{= \frac{5}{6}}
\end{aligned}
$
\end{tabular}\\

\textbf{Vérification :} $E (\mathcal{X . Y}) = E (\mathcal{X}) E (\mathcal{Y})$\\
$\frac{7}{12} = \frac{7}{10} \times \frac{5}{6}$ \textbf{OK} \checkmark\\
\textbf{Mais :}\\
$
\begin{aligned}
P ((\mathcal{X} = 0) \cap (\mathcal{Y} = 2)) = 0 \ne P (\mathcal{X} = 0) . P (\mathcal{Y} = 2) &= \frac{3}{10} \times \frac{1}{6} \\
&= \frac{1}{20}
\end{aligned}
$\\
\textbf{$\Rightarrow \mathcal{X}$ et $\mathcal{Y}$ ne sont pas indépendants.}
\end{flushleft}


\begin{rappel}{Rappel}
$
\begin{aligned}
E (\mathcal{X}) &= \underset{i \in \mathcal{I}}{\sum} x_i P (\mathcal{X} = x_i) \\
&= 0 \times \frac{3}{10} + 1 \times \frac{7}{10} \\
&= \frac{7}{10}
\end{aligned}
$
\end{rappel}



\subsection{Covariance et coefficient de corrélation linéaire}
\begin{definition}{Définition - Covariance d'un couple $(\mathcal{X, Y})$}
On appelle covariance du couple $(\mathcal{X, Y})$ :\\
$COV(\mathcal{X, Y}) = E(\mathcal{X . Y}) - E (\mathcal{X}) E (\mathcal{Y})$\\
et le coefficient de corrélation :\\
$\rho (\mathcal{X, Y}) = \frac{COV (\mathcal{X, Y})}{\sigma_x \sigma_y}$\\
($\sigma_x = \sqrt{V(\mathcal{X})}$, $\sigma_y = \sqrt{V(\mathcal{Y})}$)
\end{definition}

\begin{remarque}{Remarque}
$E$ : espace des v.a. (variables aléatoires)\\
$\rho$ : rô\\
$\cos (\theta) = \rho (\mathcal{X, Y}) = \frac{<\mathcal{X} - E (\mathcal{X}), \mathcal{Y} - E (\mathcal{Y})>}{|| \mathcal{X} - E (\mathcal{X}) || \times || \mathcal{Y} - E (\mathcal{Y}) ||}$\\
$COV (\mathcal{X, Y}) = <\mathcal{X} - E (\mathcal{X}), \mathcal{Y} - E (\mathcal{Y})>$ (produit scalaire)\\
$|| \mathcal{X} - E (\mathcal{X}) || = \sigma(\mathcal{X})$\\
$|| \mathcal{Y} - E (\mathcal{Y}) || = \sigma(\mathcal{Y})$\\
\begin{center}
\includegraphics[width=0.35\textwidth]{schémas/angle-theta.png}
\end{center}
$|\rho| \leq 1$\\
si $\rho = 1 (\theta = 0 [2\pi]) \Rightarrow$ forte corrélation linéaire\\
si $\rho = 0 (\theta = \frac{\pi}{2} [\pi]) \Rightarrow$ forte corrélation linéaire\\

\end{remarque}

\subsection{Exercice 1}
Soit $\mathcal{X}$ v.a. de la loi :\\
{\Large
\begin{center}
\begin{tabular}{c|ccccc}
	$x_i$ & -2 & -1 & 0 & 1 & 2 \\
	\hline
	$P_i$ & $\frac{1}{6}$ & $\frac{1}{4}$ & $\frac{1}{6}$ & $\frac{1}{4}$ & $\frac{1}{6}$ \\
\end{tabular}\\
\end{center}
}
Soit $\mathcal{Y} = \mathcal{X}^2$
\subsubsection{Donner la loi du couple $\mathcal{X, Y}$}
\begin{flushleft}
$P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) = 0$ si $j \neq i^2$\\
$P ((\mathcal{X} = i) \cap (\mathcal{Y} = i^2)) = P (\mathcal{X} = i)$
\end{flushleft}
{
\Large
\begin{center}
\begin{tabular}{c|ccc|c}
	$\mathcal{X/Y}$ & 0 & 1 & 4 & $P_{i \bullet}$ \\
	\hline
	-2 & 0 \tiny{[0]} & 0 \tiny{[-2]} & $\frac{1}{6}$ \tiny{[-8]} & $\frac{1}{6}$ \tiny{[-2]} \\
	-1 & 0 \tiny{[0]} & $\frac{1}{4}$ \tiny{[-1]} & 0 \tiny{[-4]} & $\frac{1}{4}$ \tiny{[-1]} \\
	0 & $\frac{1}{6}$ \tiny{[0]} & 0 \tiny{[0]} & 0 \tiny{[0]} & $\frac{1}{6}$ \tiny{[0]} \\
	1 & 0 \tiny{[0]} & $\frac{1}{4}$ \tiny{[1]} & 0 \tiny{[4]} & $\frac{1}{4}$ \tiny{[1]} \\
	2 & 0 \tiny{[0]} & 0 \tiny{[2]} & $\frac{1}{6}$ \tiny{[8]} & $\frac{1}{6}$ \tiny{[2]} \\
	\hline
	$P_{\bullet j}$ & $\frac{1}{6}$ \tiny{[0]} & $\frac{1}{2}$ \tiny{[1]} & $\frac{1}{3}$ \tiny{[4]} & 1 \\
\end{tabular}\\
\end{center}
}


\subsubsection{En déduire la loi marginale de $\mathcal{Y}$}
Loi de $\mathcal{Y} \to$ dernière ligne du tableau


\subsubsection{Indépendance et calculer $COV (\mathcal{X, Y})$}
Contre exemple pour $\mathcal{X} = 0$ et $\mathcal{Y} = 1$.\\
$P ((\mathcal{X} = 0) (\mathcal{Y} = 1)) = 0 \neq P (\mathcal{X} = 0) . P (\mathcal{Y} = 1)$\\
$\frac{1}{6} . \frac{1}{2} = \frac{1}{12}$\\
$\Rightarrow \mathcal{X}$ et $\mathcal{Y}$ ne sont pas indépendants.

\subsubsection{$COV (\mathcal{X, Y}) = E (\mathcal{X, Y}) - E (\mathcal{X}) E (\mathcal{Y})$}
$COV (\mathcal{X, Y}) = E (\mathcal{X, Y}) - E (\mathcal{X}) E (\mathcal{Y})$\\
$
\begin{aligned}
E (\mathcal{X . Y}) &= \underset{i, j}{\sum} x_i \times y_j \times P_{ij} \\
&= -\frac{1}{4} + \frac{1}{4} - \frac{8}{6} + \frac{8}{6} \\
&= 0
\end{aligned}\\
$
$E (\mathcal{Y}) = \frac{-2}{6} + \frac{2}{6} = 0$



\subsection{Exercice 2}
Soit $a \in \mathbb{R}^{x}_{+}$, $\mathcal{X, Y}$ 2 v.a. à valeurs dans $\mathbb{N}$.\\
$P ((\mathcal{X} = k) \cap (\mathcal{Y} = j)) = \frac{a}{2^{k + 1} (j!)}$ $\forall (k, j) \in \mathbb{N}^2$


\subsubsection{Déterminer a}
$\underset{k, j}{\sum} P ((\mathcal{X} = 4) \cap (\mathcal{Y} = j)) = 1$\\

\begin{rappel}{Rappel - $e^x$}
\begin{center}
$e^x = \overset{+\infty}{\underset{j = 0}{\sum}} \frac{x^j}{j!} (\forall x \in \mathbb{R})$\\
si on remplace x par 1 $\Rightarrow e = \overset{+\infty}{\underset{j = 0}{\sum}} \frac{1}{j!}$
\end{center}
\end{rappel}

\begin{rappel}{Rappel - Série Géométrique}
\begin{center}
$\overset{+\infty}{\underset{n = 0}{\sum}} a^n = \frac{1}{1 - a}$ si $|a| < 1$
\end{center}
\end{rappel}

$
\begin{aligned}
\overset{+\infty}{\underset{k = 0}{\sum}} \overset{+\infty}{\underset{j = 0}{\sum}} \frac{a}{2^{k+1} j!} = 1 &\Leftrightarrow a \overset{+\infty}{\underset{k = 0}{\sum}} \overset{+\infty}{\underset{j = 0}{\sum}} \frac{1}{2^{k+1} j!} = 1 \\
&\Leftrightarrow a \overset{+\infty}{\underset{k = 0}{\sum}} \frac{1}{2^{k+1}} \overset{+\infty}{\underset{j = 0}{\sum}} \frac{1}{j!} = 1 \\
&\Rightarrow ae \overset{+\infty}{\underset{k = 0}{\sum}} \frac{1}{2^{k+1}} = 1 \\
&\Rightarrow ae \frac{1}{2} \overset{+\infty}{\underset{k = 0}{\sum}} \frac{1}{2^k} = 1 \\
&\Rightarrow a \frac{e}{2} (\frac{1}{1 - \frac{1}{2}}) = 1 \\
&\Rightarrow ae = 1 \\
&\boxed{\Rightarrow a = e^{-1}} \\
\end{aligned}
$


\subsubsection{Déterminer les lois marginales de $\mathcal{X}$ et $\mathcal{Y}$}
\textbf{Loi marginale de $\mathcal{X}$ :}
$
\begin{aligned}
P (\mathcal{X} = k) &= \overset{+\infty}{\underset{j = 0}{\sum}} P ((\mathcal{X} = k) \cap (\mathcal{Y} = j)) \\
&= \frac{e^{-1}}{2^{k+1}} \overset{+\infty}{\underset{j = 0}{\sum}} \frac{1}{j!} \\
&= \frac{e^{-1} \times e}{2^{k+1}} \\
&\boxed{= \frac{1}{2^{k+1}} (\forall k \in \mathbb{N})} \\
\end{aligned}\\
$

\bigskip

\textbf{Loi marginale de $\mathcal{Y}$ :}
$
\begin{aligned}
P (\mathcal{Y} = j) &= \overset{+\infty}{\underset{k = 0}{\sum}} P ((\mathcal{X} = k) \cap (\mathcal{Y} = j)) \\
&= \frac{e^{-1}}{j!} \overset{+\infty}{\underset{k = 0}{\sum}} \frac{1}{2^{k+1}} \\
&= \frac{e^{-1}}{j! \times 2} (\frac{1}{1 - \frac{1}{2}}) \\
&\boxed{= \frac{e^{-1}}{j!}}
\end{aligned}
$


\subsubsection{Indépendance ?}
Il faut montrer que le produit des lois marginales est égale à la loi conjointe. En d'autre termes : $P (\mathcal{X} = k) \times P (\mathcal{Y} = j) = P ((\mathcal{X} = k) \cap (\mathcal{Y} = j))$.\\

$\frac{1}{2^{k+1}} \times \frac{e^{-1}}{j!} = \frac{e^{-1}}{2^{k+1} \times j!} \Rightarrow$ \textbf{OK} \checkmark \\
Les deux variables sont indépendantes.

\begin{remarque}{Remarque}
$\mathcal{X, Y}$ sont indépendantes $\Rightarrow E (\mathcal{X, Y}) = E (\mathcal{X}) E (\mathcal{Y})$
\end{remarque}


\subsubsection{Calculer $COV(\mathcal{X, Y})$}
$COV (\mathcal{X, Y}) = E (\mathcal{X, Y}) - E (\mathcal{X}) \times E (\mathcal{Y}) = 0$


\subsection{Exercice 3}
On considère $n$ boites numérotées de 1 à $n$. Le boite numéro $k$ contient $k$ boules numérotées de 1 à $k$. On choisit une boîte au hasard, puis on choisie une boule dans cette boîte.\\
$\mathcal{X}$ v.a. : numéro de la boîte\\
$\mathcal{Y}$ v.a. : numéro de la boule

\subsubsection{Déterminer la loi du couple ($\mathcal{X, Y}$)}
Valeurs possible de $\mathcal{X}$ et $\mathcal{Y}$ :
{
\Large
\begin{center}
\begin{tabular}{c|c}
	$\mathcal{X}$ & $\mathcal{Y}$ \\
	\hline
	1 & 1 \\
	2 & 1 \\
	2 & 2 \\
	3 & 1 \\
	3 & 2 \\
	3 & 3 \\
\end{tabular}\\
\end{center}
}

$\mathcal{X} (\Omega) = \mathcal{Y} (\Omega) = \ldbrack 1, n \rdbrack$\\
$P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) = P (\mathcal{Y} = j / \mathcal{X} = i) \times P (\mathcal{X} = i) (\forall (i, j) \in \ldbrack 1, n \rdbrack^2)$\\
\underline{1\up{er} cas :} Quand $j > i \Rightarrow P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) = 0$\\
\underline{2\up{ème} cas :} Quand $j \leq i \Rightarrow P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) = \frac{1}{i} \times \frac{1}{n}$\\


\subsubsection{Calculer $P (\mathcal{X} = \mathcal{Y})$}
$\mathcal{(X = Y)} = \overset{n}{\underset{i = 1}{\bigcup}} ((\mathcal{X} = i) \cap (\mathcal{Y} = i))$

\begin{remarque}{Remarque}
$(\mathcal{X} = i) \cap (\mathcal{Y} = i)$ est un évènement incompatible.
\end{remarque}

$
\begin{aligned}
\mathcal{P (X = Y)} &= \overset{n}{\underset{i = 1}{\sum}} P ((\mathcal{X} = i) \cap (\mathcal{Y} = i)) \\
&=  \overset{n}{\underset{i = 1}{\sum}} \frac{1}{i \times n} \\
&= \frac{1}{n}  \overset{n}{\underset{i = 1}{\sum}} \frac{1}{i}
\end{aligned}
$


\subsubsection{Donner la loi marginale de $\mathcal{Y} et E (\mathcal{Y})$}
$
\begin{aligned}
\mathcal{P (Y} = j) &= \overset{n}{\underset{i = 1}{\sum}} P((\mathcal{X} = i) \cap (\mathcal{Y} = j)) \\
&= \overset{n}{\underset{i \geq j}{\sum}} \frac{1}{i \times n} (\forall j \in \ldbrack 1, n \rdbrack) \\
&= \frac{1}{n}  \overset{n}{\underset{i \geq j}{\sum}} \frac{1}{i}
\end{aligned}
$

\begin{remarque}{Remarque - Suite Arithmétique}
{
\Large
\begin{center}
$\overset{i}{\underset{j = 1}{\sum}} j = \frac{i (i + 1)}{2}$
\end{center}
}
\end{remarque}

$
\begin{aligned}
E (\mathcal{Y}) &= \overset{n}{\underset{j = 1}{\sum}} j P (\mathcal{Y} = j) \\
&= \overset{n}{\underset{j = 1}{\sum}} \overset{n}{\underset{i \geq j}{\sum}} \frac{1}{i \times n} \\
&= \frac{1}{n} \overset{n}{\underset{j = 1}{\sum}} (j \overset{n}{\underset{i \geq j}{\sum}} \frac{1}{i}) \\
&= \frac{1}{n} \overset{n}{\underset{i = 1}{\sum}} (\frac{1}{i} \overset{i}{\underset{j = 1}{\sum}} j) \\
&= \frac{1}{n} \overset{n}{\underset{i = 1}{\sum}} (\frac{1}{i} \times \frac{i (i + 1)}{2}) \\
&= \frac{1}{n} \overset{n}{\underset{i = 1}{\sum}} \frac{i + 1}{2} \\
&= \frac{1}{2n} (\frac{n(n + 1)}{2} + n) \\
&= \frac{1}{2} (\frac{n + 1}{2} + 1) \\
&\boxed{= \frac{n + 3}{4}}
\end{aligned}
$


\subsection{Exercice 4}
Une urne contient une boule blanche et une boule noire. On y prélève une boule. Chaque boule ayant la même probabilité d'être tiré, on note sa couleurs et on la remet dans l'urne avec $c$ boules de la même couleur. On répète cette expérience $n$ fois ($n \geq 2$). $c$ est une constante fixé qui ne change pas.\\
$\mathcal{X}_i
\left \{
\begin{array}{l}
1 \text{ si on obtiens une boule blanche au } i\text{\up{ème} tirage} \\
0 \text{ sinon}
\end{array}
\right .
$
\subsubsection{La loi du couple ($\mathcal{X}_1, \mathcal{X}_2$) et en déterminer la loi de $\mathcal{X}_2$}
$P (\mathcal{X}_1 = 1) = P (\mathcal{X}_1 = 0) = \frac{1}{2}$ \\
$\mathcal{X}_i$ suis la variable de Bernoulli $\mathcal{B} (\frac{1}{2})$ \\
$P ((\mathcal{X}_1 = i) \cap (\mathcal{X}_2 = j)) = P (\mathcal{X}_2 = j / \mathcal{X}_1 = i) \times P (\mathcal{X}_1 = i)$ \\

\begin{flushleft}
\underline{1\up{er} cas $i \neq j$ :} $P ((\mathcal{X}_1 = i) \cap (\mathcal{X}_2 = j)) = \frac{1}{c + 2} \times \frac{1}{2}$ \\
\underline{2\up{ème} cas $i = j$ :} \\
$
\begin{aligned}
P ((\mathcal{X}_1 = i) \cap (\mathcal{X}_2 = i)) &= P (\mathcal{X}_2 = i / \mathcal{X}_1 = i) \times P (\mathcal{X}_1 = i) \\
&= \frac{1 + c}{2 + c} \times \frac{1}{2}
\end{aligned}
$
\end{flushleft}

{
\Large
\begin{center}
\begin{tabular}{c|cc|c}
	$\mathcal{X}_1$ / $\mathcal{X}_2$ & 0 & 1 & loi $\mathcal{X}_1$ \\
	\hline
	0 & $\frac{1 + c}{2 (2 + c)}$ & $\frac{1}{2 (2 + c)}$ & $\frac{1}{2}$ \\
	1 & $\frac{1}{2 (2 + c)}$ & $\frac{1 + c}{2 (2 + c)}$ & $\frac{1}{2}$ \\
	\hline
	loi $\mathcal{X}_2$ & $\frac{1}{2}$ & $\frac{1}{2}$ & 1 \\

\end{tabular}\\
\end{center}
}

\underline{D'après le tableau :} $\mathcal{X}_2$ suis la variable $\mathcal{B} (\frac{1}{2})$ (Bernoulli).


\subsubsection{Déterminer la loi de $\mathcal{Z}_2$}
\begin{flushleft}
$\mathcal{Z}_p = \overset{p}{\underset{p}{\sum}} \mathcal{X}_i \quad (2 \leq p \leq n)$ \\
$\mathcal{Z}_p$ : nombre de boules blanches obtenue lors des premiers tirages. \\
$\mathcal{Z}_2 = \mathcal{X}_1 + \mathcal{X}_2$ \\
$\mathcal{Z}_2 (\Omega) = \{0, 1, 2\}$ \\
$P (\mathcal{Z}_2 = 0) = P ((\mathcal{X}_1 = 0) \cap (\mathcal{X}_2 = 0)) = \frac{1 + c}{2 (2 + c)}$ \\
$
\begin{aligned}
P (\mathcal{Z}_2 = 1) &= P ((\mathcal{X}_1 = 0) \cap (\mathcal{X}_2 = 0)) + P ((\mathcal{X}_1 = 1) \cap (\mathcal{X}_2 = 0)) \\
&= \frac{1}{2 + c}
\end{aligned}
$

$
\begin{aligned}
P (\mathcal{Z}_2 = 2) &= P ((\mathcal{X}_1 = 1) \cap (\mathcal{X}_2 = 1)) \\
&= \frac{1 + c}{2 (2 + c)}
\end{aligned}
$
\end{flushleft}


\subsubsection{Déterminer $\mathcal{Z}_p(\Omega)$ et calculer $P (\mathcal{X}_{p + 1} = 1 / \mathcal{Z}_p = k)$}
$P (\mathcal{X}_{p + 1} = 1 / \mathcal{Z}_p = k)$ et $\mathcal{Z}_p = \overset{p}{\underset{i = 1}{\sum}} \mathcal{X}_i$ \\
$(\mathcal{Z}_p = k) \Leftrightarrow$ Evenement \og{} \textit{Au cours des $p$ premier tirage on a obtenue k boules blanches et $(p - k)$ boules noires.}\fg{} \\
Avant de passer au $(p + 1)$\up{ième} tirage l'urne contiens : $kc + (p - k)c + 2 = 2 + pc$ dont $1 + kc$ boules blanches. \\
Ainsi : $P (\mathcal{X}_{p + 1} = 1 / \mathcal{Z}_p = k) = \frac{1 + kc}{2 + pc}$


\subsubsection{Montrer que $P (\mathcal{X}_{p + 1} = 1) = \frac{1 + cE(\mathcal{Z}_p)}{2 + pc}$}
$(\mathcal{X}_{p + 1} = 1) = \overset{p}{\underset{k = 0}{\bigcup}} ((\mathcal{X}_{p + 1} = 1) \cap (\mathcal{Z}_p = k))$
\begin{remarque}{Remarque}
$(\mathcal{X}_{p + 1} = 1) \cap (\mathcal{Z}_p = k)$ est un évènement incompatible.
\end{remarque}

$
\begin{aligned}
P (\mathcal{X}_{p + 1} = 1) &= \overset{p}{\underset{k = 0}{\sum}} P ((\mathcal{X}_{p + 1} = 1) \cap (\mathcal{Z}_{p} = k)) \\
&= \overset{p}{\underset{k = 0}{\sum}} P (\mathcal{X}_{p + 1} = 1 / \mathcal{Z}_p = k) \times P (\mathcal{Z}_p = k) \\
&= \overset{p}{\underset{k = 0}{\sum}} (\frac{1 + kc}{2 + pc}) \times P (\mathcal{Z}_p = k) \\
&= \frac{1}{2 + pc} \times (\overset{p}{\underset{k = 0}{\sum}} P (\mathcal{Z}_p = k) + c \overset{p}{\underset{k = 0}{\sum}} k P (\mathcal{Z}_p = k)) \\
&\boxed{= \frac{1 + c E(\mathcal{Z}_p)}{2 + pc}}
\end{aligned}
$


\subsubsection{Montrer que $P (\mathcal{X}_p = 1) = P (\mathcal{X}_p = 0) = \frac{1}{2} \quad (\forall p \in \ldbrack 1, n \rdbrack)$}
\begin{rappel}{Rappel}
{
\Large
\begin{center}
$E(\mathcal{X}) = \underset{k}{\sum} k P (\mathcal{X} = k)$
\end{center}
}
\end{rappel}

Soit $R_p$ la propriété  : $P (\mathcal{X}_p = 1) = P (\mathcal{X}_p = 0) = \frac{1}{2}$. \\
$R_1$ et $R_2$ sont vérifié (1\up{ère} question). \\

\begin{rappel}{Rappel - Bernoulli}
Soit $\mathcal{X}_i$ suis $\mathcal{B} (q)$ \\
Alors : $E (\mathcal{X}_i) = q$, $V (\mathcal{X}_i) = q (1 - q)$
\end{rappel}

\underline{Hypothèse :} Supposons $R_k$ vraie $\forall 1 \leq k \leq p$ \\
$P (\mathcal{X}_{p + 1} = 1) = \frac{1 + c E(\mathcal{Z}_p)}{2 + pc}$ \\
$\mathcal{Z}_p = \overset{p}{\underset{i = 1}{\sum}} \mathcal{X}_i \Rightarrow E(\mathcal{Z}_p) = \overset{p}{\underset{i = 1}{\sum}} E (\mathcal{X}_i)$ \\
$P (\mathcal{X}_{p + 1} = 1) = \frac{1 + \frac{cp}{2}}{2 + pc} = \frac{1}{2}$ \\
$P (\mathcal{X}_{p + 1} = 0) = 1 - \frac{1}{2} = \boxed{\frac{1}{2}}$ \\

\underline{Conclusion :} $R_p$ est vraie $\forall 1 \leq p \leq n$



\subsection{Exercice 5}
Une urne contiens des boules noires en proportion $p$ ($1 \leq p \leq 1$) et des boules blanches en proportion $q = 1 - p$. On effectue une suite de tirage d'une boule avec remise.

\subsubsection{Question 1 : loi marginale et indépendance}
\textbf{On note $N$ le rang aléatoire d'apparition de la première boule noire et $B$ celui de la première boule blanche.}

\paragraph{a) Déterminer les lois de $N$ et $B$, $E(N)$, $V(N)$, $E(B)$, $V(B)$ \\}
$
\left \{
\begin{array}{l}
N \text{: temps d'attente de la réalisation de la 1\up{ère} boule noire} \\
B \text{: temps d'attente de la réalisation de la 1\up{ère} boule blanche}
\end{array}
\right .
$

$N$ suis la loi géométrique de paramètre $p$ (autrement dit : $N \nearrow \mathcal{G} (p)$) \\

\begin{flushleft}
$P (N = k) = (1 - p)^{k - 1} \times p$ \\
$N(\Omega) = \ldbrack 1, +\infty \ldbrack$ \\
$B(\Omega) = \ldbrack 1, +\infty \ldbrack$ \\
$P (B = k) = (1 - q) \times q$ \\
$B \nearrow \mathcal{G}(q)$ pour $q = 1 - p$ \\
$E(N) = \frac{1}{p} \quad V(N) = \frac{q}{p^2}$ \\
$E(B) = \frac{1}{q} \quad V(B) = \frac{p}{q^2}$ \\
$P (N = k) = q^{k - 1} \times p \quad (\forall k \in \mathbb{N}^*)$ \\
$P (B = k) = p^{k - 1} \times q \quad (\forall k \in \mathbb{N}^*)$ \\
\end{flushleft}


\paragraph{b) $N$ et $B$ sont-elle indépendante ? \\}
\begin{flushleft}
$P ((N = 1) \cap (B = 1)) = 0 \quad (N = 1) \cap (B = 1) = \varnothing$ \\
$P(N = 1) \times P (B = 1) = pq \neq 0$ \\
Donc $N$ et $B$ ne sont pas indépendantes.
\end{flushleft}


\subsubsection{Question 2 : loi conjointe}
\textbf{On note $\mathcal{X}$ la longueur de la 1\up{ère} suite de boule de a même couleur et $\mathcal{Y}$ celle de la 2\up{ème} suite de boule de la même couleur.}

\begin{flushleft}
Exemple : $(\mathcal{X} = 1) \cap (\mathcal{Y} = 2)$ : $(B_1 \cap N_2 \cap N_3 \cap B_4) \cup (N_1 \cap B_2 \cap B_3 \cap N_4)$
\end{flushleft}


\paragraph{a) Déterminer la loi conjointe de $(\mathcal{X, Y})$ \\}
\begin{flushleft}
$\mathcal{X} (\Omega) = \mathcal{Y} (\Omega) = \mathbb{N}^*$ \\
$(\mathcal{X} = i) \cap (\mathcal{Y} = j)$ : \\
$(N_1 \cap \cdots \cap N_i \cap B_{i + 1} \cap \cdots \cap B_{i + j} \cap N_{i + j + 1}) \cup (B_1 \cap \cdots \cap B_i \cap N_ {i + 1} \cap \cdots \cap N_{i + j} \cap B_{i + j + 1})$ \\
$\boxed{P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) = p^{i + 1} \times q^j + q^{i + 1} \times p^j}$
\end{flushleft}


\paragraph{b) Loi de $\mathcal{X}$? $E(\mathcal{X})$ et $E(\mathcal{X}) \geq 2$ \\}
\begin{flushleft}
\begin{rappel}{Rappel}
{
\Large
\begin{center}
$\overset{+\infty}{\underset{j = 0}{\sum}} \mathcal{X}^j = \frac{1}{1 - \mathcal{X}} \quad |\mathcal{X}| < 1$
\end{center}
}
\end{rappel}
\end{flushleft}

$
\begin{aligned}
\forall i \in \mathbb{N}^* \quad P (\mathcal{X} = i) &= \overset{+\infty}{\underset{j = 1}{\sum}} (p^{i + 1} \times q^j + q^{i + 1} \times p^j) \\
&= p^{i + 1} \overset{+\infty}{\underset{j = 1}{\sum}} q^j + q^{i + 1} \overset{+\infty}{\underset{j = 1}{\sum}} p^j \\
&= \frac{p^{i + 1} q}{1 - q} + \frac{q^{i + 1} p}{1 - p} \\
&\boxed{= p^iq + q^ip}
\end{aligned}
$

\begin{rappel}{Rappel}
$f(p) = \overset{+\infty}{\underset{n = 0}{\sum}} p^n = \frac{1}{1 - p} \quad (|p| < 1)$ \\
$f'(p) = \overset{+\infty}{\underset{n = 1}{\sum}} np^{n - 1} = \frac{1}{(1 - p)^2}$
\end{rappel}

\begin{flushleft}
$P (\mathcal{X} = i) = p^iq + q^ip$ \\
$
\begin{aligned}
E(\mathcal{X}) &= \overset{+\infty}{\underset{n \geq 1}{\sum}} n \times P (\mathcal{X} = n) \\
&= \overset{+\infty}{\underset{n \geq 1}{\sum}} (np^nq + nq^np) \\
&= q \overset{+\infty}{\underset{n \geq 1}{\sum}} np^n + p \overset{+\infty}{\underset{n \geq 1}{\sum}} nq^n \\
&= \frac{qp}{(1 - p)^2} + \frac{pq}{(1 - q)^2} \\
&= \frac{p}{q} + \frac{q}{p} \\
&\boxed{= \frac{p^2 + q^2}{pq}}
\end{aligned}
$

$
\begin{aligned}
\text{or }(p - q)^2 \geq 0 &\Rightarrow p^2 + q^2 - 2pq \geq 0 \\
&\Rightarrow \frac{p^2 + q^2}{pq} \geq 2 \\
&\Rightarrow E(\mathcal{X}) \geq 2
\end{aligned}
$
\end{flushleft}


\paragraph{c) Loi de $\mathcal{Y}$? $E(\mathcal{Y})$ et $E(\mathcal{Y})$ \\}
\begin{flushleft}
$
\begin{aligned}
\forall j \in \mathbb{N}^* \quad P (\mathcal{Y} = j) &= \overset{+\infty}{\underset{i = 1}{\sum}} (p^{i + 1} q^j + q^{i + 1} p^j) \\
&= q^j p \overset{+\infty}{\underset{i = 1}{\sum}} p^i + qp^i \overset{+\infty}{\underset{i = 1}{\sum}} q^i \\
&= q^jp^2 \frac{1}{1 - p} + \frac{q^2 p^j}{1 - q} \\
&= p^2q^{j - 1} + q^2p^{j - 1} \\
&\boxed{= p^2 q^{j - 1} + q^2 p^{j - 1}}
\end{aligned}
$

$
\begin{aligned}
E(\mathcal{Y}) &= \underset{n \geq 1}{\sum} n p^2 q^{n - 1} + \underset{n \geq 1}{\sum} n q^2 p^{n - 1} \\
&= p^2 \underset{n \geq 1}{\sum} n q^{n - 1} + q^2 \underset{n \geq 1}{\sum} n p^{n - 1} \\
&= \frac{p^2}{(1 - q)^2} + \frac{q^2}{(1 - p)^2} \\
&= 1 + 1 \\
&\boxed{= 2}
\end{aligned}
$

$V(\mathcal{Y}) = E(\mathcal{Y}^2) - E^2(\mathcal{Y})$ \\

\begin{rappel}{Rappel}
$\underset{n \geq 1}{\sum} np^n = \frac{p}{(1 - p)^2} \quad$ (en dérivant cette fonction en fonction de $p$). \\

$
\begin{aligned}
\underset{n \geq 1}{\sum} n^2 p^{n - 1} &= \frac{(1 - p)^2 + 2p (1 - p)}{(1 - p)^4} \\
&= \frac{1 - p + 2p}{(1 - p)^3} \\
&\boxed{= \frac{1 + p}{(1 - p)^3}}
\end{aligned}
$
\end{rappel}

$
\begin{aligned}
E(\mathcal{Y}^2) &= \underset{n \geq 1}{\sum} n^2 P (\mathcal{Y} = n) \\
&= \underset{n \geq 1}{\sum} n^2 p^2 q^{n - 1} + \underset{n \geq 1}{\sum} n^2 q^2 p^{n - 1} \\
&= p^2 \underset{n \geq 1}{\sum} n^2 q^{n - 1} + q^2 \underset{n \geq 1}{\sum} n^2 p^{n - 1} \\
&= p^2 \left (\frac{1 + q}{(1 - q)^3} \right ) + q^2 \left (\frac{1 + p}{(1 - p)^3} \right ) \\
&= \frac{1 + q}{p} + \frac{1 + p}{q} \\
&\boxed{= \frac{2q}{p} + \frac{2p}{q} + 2}
\end{aligned}
$

$
\begin{aligned}
V(\mathcal{Y}) &= \frac{2q}{p} + \frac{2p}{q} + 2 - 4 \\
&\boxed{= 2 (\frac{q}{p} + \frac{p}{q} - 1)}
\end{aligned}
$

\end{flushleft}


\paragraph{d) Calculer $P (\mathcal{X} = \mathcal{Y})$ \\}
$P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) = p^{i + 1} q^j + q^{i + 1} p^j$ \\
$(\mathcal{X} = \mathcal{Y}) = \overset{+\infty}{\underset{n = 1}{\bigcup}} ((\mathcal{X} = n) \cap (\mathcal{Y} = n))$ \\

\begin{note}{Note}
$(\mathcal{X} = n) \cap (\mathcal{Y} = n)$ est in évènement indépendant.
\end{note}

$
\begin{aligned}
P (\mathcal{X} = \mathcal{Y}) &= \overset{+\infty}{\underset{n = 1}{\sum}} P ((\mathcal{X} = n) \cap (\mathcal{Y} = n)) \\
&= \overset{+\infty}{\underset{n = 1}{\sum}} (p^{n + 1} q^n + q^{n + 1} p^n) \\
&= p \overset{+\infty}{\underset{n = 1}{\sum}} (pq)^n + q \overset{+\infty}{\underset{n = 1}{\sum}} (pq)^n \\
&= p^2 q \left (\frac{1}{1 - pq} \right ) + pq^2 \left (\frac{1}{1 - pq} \right ) \\
&= \frac{p^2q + pq^2}{1 - pq} \\
&= \frac{pq (p + q)}{1 - pq} \\
&\boxed{= \frac{pq}{1 - pq}} \\
\end{aligned}
$

\begin{remarque}{Remarque}
Dans le cas ou on a 2 variables souvent on obtiens une série.
\end{remarque}


\paragraph{e) Calculer la loi de $\mathcal{X + Y} \quad (p = \frac{1}{2})$ \\}
$\mathcal{X} (\Omega) = \mathcal{Y} (\Omega) = \mathbb{N}^*$ \\

\begin{note}{Note}
{
\Large
\begin{center}
$\overset{+\infty}{\underset{n = 0}{\sum}} p^n = \frac{1}{1 - p}$
\end{center}
}
\end{note}

$(\mathcal{X} + \mathcal{Y})(\Omega) = \ldbrack 2, +\infty \ldbrack$ \\

On cherche a prouver : $\forall k \in \ldbrack 2, +\infty \ldbrack \quad P (\mathcal{X} + \mathcal{Y} = k)$ \\

$
\begin{aligned}
P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) &= \frac{1}{2^{i + 1}} \times \frac{1}{2^j} + \frac{1}{2^{i + 1}} \times \frac{1}{2^j} \\
&= \left ( \frac{1}{2} \right )^{i + j}
\end{aligned}
$

$
\begin{aligned}
P (\mathcal{X} + \mathcal{Y} = k) &= \underset{(i, j) / i + j = k}{\sum} P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) \\
&= \overset{k - 1}{\underset{i = 1}{\sum}} P ((\mathcal{X} = i) \cap (\mathcal{Y} = k - i)) \\
&= \overset{k - 1}{\underset{i = 1}{\sum}} \left ( \frac{1}{2} \right )^{k} \\
&\boxed{= (k - 1) \left ( \frac{1}{2} \right )^{k}} \\
\end{aligned}
$

\subsection{Exercice 6}
Soit $a \in ] 0, 1 [$ et $b \in ] 0, +\infty [$. $\mathcal{X}$ et $\mathcal{Y}$ deux v.a. dont la loi conjointe est : \\
$P ((\mathcal{X} = i) \cap (\mathcal{Y} = j))
\left \{
\begin{array}{lc}
0 & i < j \\
\frac{b^i e^{-b} a^j (1 - a)^{i - j}}{j! (i - j)!} & i \geq j
\end{array}
\right .
$\\
$\mathcal{X} (\Omega) = \mathcal{Y} (\Omega) = \mathbb{N}$

\subsubsection{Déterminer les loi marginales ainsi que $E(\mathcal{X}), V(\mathcal{X}), E(\mathcal{Y}), V(\mathcal{Y})$}
\begin{note}{Note}
{
\Large
\begin{center}
$
\left (
\begin{array}{c}
i \\
j \\
\end{array}
\right )
 = \frac{i!}{j! (i - j)!}
$
\end{center}
}
\end{note}

\begin{rappel}{Rappel - Formule du Binôme de Newton}
{
\Large
\begin{center}
$(a + b)^n = \overset{n}{\underset{k = 0}{\sum}} \left ( \begin{array}{c} n \\ k \end{array} \right ) a^k b^{n - k}$
\end{center}
}
\end{rappel}

$
\begin{aligned}
P (\mathcal{X} = i) &= \underset{j \in \mathbb{N}}{\sum} P ((\mathcal{X} = i) \cap (\mathcal{Y} = j)) \\
&= \overset{i}{\underset{j = 0}{\sum}} \frac{b^i e^{-b} a^j (1 - a)^{i - j}}{j! (i - j)!} \\
&= b^i e^{-b} \overset{i}{\underset{j = 0}{\sum}} \frac{a^j (1 - a)^{i - j}}{j! (i - j)!} \\
&= \frac{b^i e^{-b}}{i!}  \overset{i}{\underset{j = 0}{\sum}} \left ( \begin{array}{c} i \\ j \end{array} \right ) a^j (1 - a)^{i - j} \\
&= \frac{b^i e^{-b}}{i!} (a + 1 - a)^i \quad \text{(Formule du Binôme de Newton)} \\
&\boxed{= e^{-b} \frac{b^i}{i!} \quad \forall i \in \mathbb{N}}
\end{aligned}
$ \\

\begin{rappel}{Rappel - Loi de Poisson}
On dit que $\mathcal{X}$ suit $\mathcal{P} (\lambda)$ la loi de Poisson :
{
\Large
\begin{center}
$\boxed{P (X = k) = e^{-k} \frac{\lambda^k}{k!} \quad \forall k \in \mathbb{N}}$ \\
\end{center}
}
$E(\mathcal{X}) = V(\mathcal{X}) = \lambda$
\end{rappel}

$\mathcal{X} \nearrow \mathcal{P} (b)$ ainsi $E(\mathcal{X}) = V(\mathcal{X}) = \lambda \quad \forall j \in \mathcal{N}$ \\

\begin{rappel}{Rappel}
{
\Large
\begin{center}
$e^n = \overset{+\infty}{\underset{k = 0}{\sum}} \frac{nk}{k!} \quad \forall n \in \mathbb{R}$
\end{center}
}
\end{rappel}

$
\begin{aligned}
P(\mathcal{Y} = j) &= \underset{i \in \mathbb{N}}{\sum} P((\mathcal{X} = i) \cap (\mathcal{Y} = j)) \\
&= \overset{+\infty}{\underset{i = j}{\sum}} \frac{b^i e^{-b} a^j (1 - a)^{i - j}}{j! (i - j)!} \\
&= \frac{e^{-b} a^j}{j!} \overset{+\infty}{\underset{i = j}{\sum}} \frac{b^i (1 - a)^{i - j}}{(i - j)!} \\
&= \frac{e^{-b} (ab)^j}{j!} \overset{+\infty}{\underset{i = j}{\sum}} \frac{(b(1 - a))^{i - j}}{(i - j)!} \\
&= \frac{e ^{-b} (ab)^j}{j!} \times e^{b (1 - a)} \\
&\boxed{= \frac{(ab)^j}{j!} \times e^{-ab} }
\end{aligned}
$\\

Donc $\mathcal{Y} \nearrow \mathcal{P} (ab)$. 


\subsubsection{$\mathcal{X}$ et $\mathcal{Y}$ sont-elles indépendantes ?}
\begin{flushleft}
$\mathcal{X} \nearrow \mathcal{P} (b) \quad P((\mathcal{X} = 0) \cap (\mathcal{Y} = 1)) = 0$ \\
$P(\mathcal{X} = 0) \times P(\mathcal{Y} = 1) = e^{-b} ab e^{-ab} \neq 0$ \\
$\mathcal{X}$ et $\mathcal{Y}$ ne sont pas indépendantes.
\end{flushleft}


\subsubsection{Déterminer la loi de $\mathcal{Z} = \mathcal{X} - \mathcal{Y}$}
$
\begin{aligned}
P(\mathcal{Z} = k) &= \underset{(i, j) / i - j = k}{\sum} P((\mathcal{X} = i) \cap (\mathcal{Y} = j)) \\
&= \overset{+\infty}{\underset{i \geq k}{\sum}} P((\mathcal{X} = i) \cap (\mathcal{Y} = i - k)) \\
&= \overset{+\infty}{\underset{i = k}{\sum}} \frac{b^i e^{-b} a^{i - k} (1 - a)^k}{(i - k)! k!} \\
&= \frac{e^{-b} (1 - a)^k}{k!} \overset{+\infty}{\underset{i = k}{\sum}} \frac{b^i a^{i - k}}{(i - k)!} \\
&= \frac{e^{-b} ((1 - a)b)^k}{k!} \overset{+\infty}{\underset{i = k}{\sum}} \frac{(ab)^{i - k}}{(i - k)!} \\
&= \frac{e^{-b} ((1 - a)b)^k}{k!} \times e^{ab} \\
&= e^{-b (1 - a) \frac{((1 - a)b)^k}{k!}}
\end{aligned}
$ \\

$\mathcal{Z} \nearrow \mathcal{P} ((1 - a)b)$.


\subsubsection{$\mathcal{Y} et \mathcal{Z}$ sont-elle indépendantes ?}
$\mathcal{Y} \nearrow \mathcal{P} (ab), \mathcal{Z} \nearrow \mathcal{P} ((1 - a)b) \quad \forall (j, k) \in \mathbb{N}$ \\

$
\begin{aligned}
P((\mathcal{Y} = j) \cap (\mathcal{Z} = k)) &= P((\mathcal{Y} = j) \cap (\mathcal{X} - \mathcal{Y} = k)) \\
&= P((\mathcal{Y} = j) \cap (\mathcal{X} = j + k)) \\
&= \frac{b^{j + k} e^{-b} a^j (1 - a)^k}{j! k!}
\end{aligned}
$ \\

$
\begin{aligned}
P(\mathcal{Y} = j) \times P(\mathcal{Z} = k) &= e^{-ab} \frac{(ab)^j}{j!} \times e^{(1 - a)b} \frac{((1 - a)b)^k}{k!} \\
&= \frac{(ab)^j e^{-b} ((1 - a)b)^k}{j! k!}
\end{aligned}
$ \\


\section{Analyse en composante principale}
\subsection{Les données et leurs caractéristiques}
\subsubsection{Tableau de donnée}
Les observations de $p$ variables sur $n$ individus sont rassemblé en une matrice $X$ à $n$ ligne et $p$ colonne : \\

\begin{flushleft}
$X =
\begin{pmatrix}
   X^{(1)}_{1} & ... & X^{(j)}_{1} & .. & X^{(p)}_{1} \\
   & & : & & \\
   x^{(1)}_{i} & ... & X^{(j)}_{i} & ... & X^{(p)}_{i} \\
   & & : & & \\
   X^{(1)}_{n} & ... & X^{(j)}_{n} & .. & X^{(p)}_{n}
\end{pmatrix}
$ \\
\medskip
$X^{(j)}_{i}$ : valeurs prises par la valeur $X^{(j)}$ sur le i\up{ème} individu. \\
\medskip
$
X^{(j)} =
\begin{pmatrix}
X^{(j)}_{1} \\
X^{(j)}_{2} \\
: \\
X^{(j)}_{n} \\
\end{pmatrix}
$ \\
\medskip
$t_{e_{i}} = (X^{(1)}_i, X^{(2)}_i, ..., X^{(p)}_i) \quad$ i\up{ème} individu. \\
\end{flushleft}


\subsubsection{Matrice de poids}
On associe à chaque individu un poids $p_i \quad (p_i \geq 0)$ (probabilité de choisir l'individu i). \\

\begin{flushleft}
$\overset{n}{\underset{i = 1}{\sum}} p_i = 1 \quad D = 
\begin{pmatrix}
p_i & & & & \text{\LARGE 0} \\
& . & & & \\
& & . & & \\
& & & .  &\\
\text{\LARGE 0} & & & & p_n \\
\end{pmatrix}
$ \\
\medskip
Si $p_i = \frac{1}{n} \quad \forall i \quad D = \frac{1}{n} I_n \quad$ ($I_n$ : Matrice Identité) \\
\end{flushleft}


\subsubsection{Centre de gravité}
Le vecteur g : $t_g = (\overline{X}^{(1)}, \overline{X}^{(2)}, .., \overline{X}^{(p)})$ \\

\begin{flushleft}
$\overline{X}^{(j)} = \overset{n}{\underset{i = 1}{\sum}} p_i X^{(j)}_{i} \quad$ Moyenne arithmétique de $X^{(j)}$. \\

Le tableau des données centré : $Y$ \\
\medskip
$\boxed{Y^{(j)}_i = X^{(j)}_i - \overline{X}^{(j)}}$
\end{flushleft}


\subsubsection{Matrice de Variance - Covariance et Matrice de Corrélation}
\begin{definition}{Définition - Matrice de Variance-Covariance}
On appelle matrice de variance-covariance : \Large $\boxed{V = \ ^tYDY}$
\end{definition}

\begin{flushleft}
Si on note $D_{\frac{1}{S}}$ la matrice diagonale des inverses des écarts-types : \\
$
D_{\frac{1}{S}} =
\begin{pmatrix}
\frac{1}{S_1} & & & & \text{\LARGE 0} \\
& \frac{1}{S_2} & & & \\
& & . & & \\
& & & . &\\
\text{\LARGE 0} & & & & \frac{1}{S_p}\\
\end{pmatrix}
$ \\
\medskip
$
\begin{aligned}
\text{où } S_j &= \sqrt{V(X^{(j)})} \quad \text{écart type} \\
&= \overset{n}{\underset{i = 1}{\sum}} p_i \left (Y^{(j)}_i \right )^2
\end{aligned}
$ \\
\medskip
$
\begin{aligned}
V(X^{(j)}) &: \text{variance de } X^{(j)} \\
S_j &: \text{écart-type de } X^{(j)}
\end{aligned}
$ \\
\medskip
On appelle la matrice des données centrées et réduites $Z$ telle que : $\boxed{Z^{(j)}_i = \frac{Y^{(j)}_i}{S_j}}$ \\
\medskip
Matriciellement : $\boxed{Z = YD_{\frac{1}{S}}}$

La matrice regroupant tous les coefficients de corrélation linéaire entre les $p$ variables est $R$ : \\
$
R = 
\begin{pmatrix}
1 & & & & l_{ij} \\
& . & & & \\
& & . & & \\
& & & . & \\
l_{ij} & & & & 1 \\
\end{pmatrix}
\text{ symétriques}
$ \\
\medskip
$r_{ij} = l_{ij} = \frac{COV (X^{(i)}, X^{(j)})}{S_i S_j} \quad$ ($l_{ij}$ : coefficient de corrélation) \\
où $COV (X^{(i)}, X^{(j)})$ : covariance \\
$COV (X^{(i)}, X^{(j)}) = \overset{n}{\underset{k = 1}{\sum}} l_k . Y^{(i)}_k . Y^{(j)}_k \quad$ (produit scalaire des variables centrées) \\

\begin{remarque}{Remarque}
$
\begin{aligned}
R &= D_{\frac{1}{S}} V D_{\frac{1}{S}} \\
&= D_{\frac{1}{S}} \ ^tYDY D_{\frac{1}{S}}
\end{aligned} \\
\Rightarrow \boxed{R = \ ^tZDZ}
$
\end{remarque}
\end{flushleft}


\subsection{Espace des individus}
Chaque individus étant un vecteur défini par $p$ coordonnées est considéré comme un élément d'un espace vectoriel $F$ appelé l'espace des individus. \\
Les $n$ individus forment alors un nuage de points dans $F$ et $g$ en est le barycentre (ou centre de gravité). \\
On munit l'espace $F$ d'une métrique (distance) : \\
$<e_i, e_j> = t_{e_i} M l_j \quad$ ($<x, y>$ : produit scalaire)
où $M$ est une matrice S.D.P (Symétrique et Définie Positive). \\

\begin{remarque}{Remarque}
Si $M = I \quad$ ($I$ : Identité), on retrouve le produit scalaire usuel. \\
Si $M = D_{\frac{1}{S^2}} = 
\begin{pmatrix}
\frac{1}{S^2_1} & & & & \text{\LARGE 0} \\
& . & & & \\
& & . & & \\
& & & . & \\
\text{\LARGE 0} & & & & \frac{1}{S^2_p} \\
\end{pmatrix}
$
cela revient à diviser chaque caractère par son écart-type.
\end{remarque}

\begin{definition}{Définition - Inertie}
On appelle inertie totale du nuage de points la moyenne pondérée des carrés des distances des points au centre de gravité : \\
$
\begin{aligned}
I_g &= \overset{n}{\underset{i = 1}{\sum}} p_i \ ^t(e_i - g) M(e_i - g) \\
&= \overset{n}{\underset{i = 1}{\sum}} p_i \|e_i - g\|^2 \\
\end{aligned} \\
$
\end{definition}

\begin{propriete}{Propriété - Inertie}
On peut montrer que l'inertie du nuage est égale à le trace de la matrice MV : \\
$\boxed{I_g = Trace (MV) = Trace (VM)}$
\end{propriete}


\subsection{Espace des variables}
\begin{flushleft}
On note $E$ : l'espace des variables
$
X^{(j)} =
\begin{pmatrix}
X^{(j)}_1 \\
: \\
X^{(j)}_n
\end{pmatrix}
$ \\
On munit $E$ de la métrique $M = D \quad$ ($D$ : matrice de poids) \\

$<X^{(j)}, X^{(k)}> = \ ^tX^{(j)} D X^{(k)}$ \\

Si les variables sont centrées on a : \\
$
\begin{aligned}
^t X^{(j)} D X^{(k)} &= \overset{n}{\underset{i = 1}{\sum}} p_i X^{(j)}_i X^{(k)}_i \\
&= COV (X^{(j)}, X^{(k)})
\end{aligned}
$

La norme de $X^{(j)} \quad$ (variable centrée) \\

$
\begin{aligned}
\|X^{(j)}\|^2 &= <X^{(j)}, X^{(j)}> \\
&= \overset{n}{\underset{i = 1}{\sum}} p_i \left (X^{(j)} \right )^2 \\
&= S^2_j
\end{aligned}
$ \\
$\Rightarrow \|X^{(j)}\| = S_j \quad$ (écart-type) \\
On mesure l'angle entre 2 variables $X^{(j)}$ et $X^{(k)}$ (centrées) : \\
$
\begin{aligned}
\cos (\theta_{jk}) &= \frac{<X^{(j)}, X^{(k)}>}{\|X^{(j)}\| \|X^{(k)}\|} \\
&= \frac{COV (X^{(j)}, X^{(k)})}{S_j S_k} \\
&= l_{ik}
\end{aligned}
$ \\
On retrouve le coefficient de corrélation linéaire.
\end{flushleft}


\subsection{Variables engendrées par un tableau de données}

$A$ une variable $X^{(j)}$, on peut associer un axe de l'espace des individus $F$ et un vecteur de l'espace des variables et on peut également déduire de $X^{(1)}, X{(2)}, ..., X^{(j)}, ..., X^{(p)}$ de nouvelle variables par combinaison linéaire. \\
Soit $\Delta$ un axe de $F$. $\Delta$ est engendré par un vecteur unitaire a $(\ ^t a M a = 1)$ et projection des individus sur $\Delta$ (projection M orthogonale).

\begin{center}
\includegraphics[width=0.8\textwidth]{schémas/projection.png}
\end{center}

$c_i = \ ^t a M e_i = \ ^t e_i Ma = <e_i, a>$ (produit scalaire) \\

La liste des coordonnées $c_i$ des individues sur $\Delta$ forme uen nouvelle variable artificielle $C$.\\

$C =
\begin{pmatrix}
C_1 \\
C_2 \\
: \\
C_n \\
\end{pmatrix}
 = X Ma = Xu$ on pose $u = Ma$ : facteur. \\
 $\Rightarrow C = Xu = \overset{p}{\underset{j = 1}{\sum}} u_j X^{(j)}$ \\
 
 Donc le nouvelle variable $C$ est une combinaison linéaire de variables initiale. L'enssemble des variables $C$ que l'on peut engendrer par combinaison linéaire des vecteurs colonne de $X$ forme un s.e.v. (sous espace vectoriel) de E de dimension $\leqslant p$. \\
 
\begin{remarque}{Remarque}
Si $M = I \Rightarrow u = a$
\end{remarque}

On suppose que les variables sont centrées $(X = Y)$ pour simplifier.

\begin{proposition}{Proposition}
$\boxed{V(C) = \ ^t u Vu} \quad (\text{variance dec})$ \\

\textbf{Démonstration :} \\
$
\begin{aligned}
V(C) &= \ ^t CDC \\
&= \ ^t (Xu) DXu \\
&= \ ^t u \ ^t XDXu \quad (\ ^t XDX = V)
\end{aligned}
$ \\
$\Rightarrow V(C) = \ ^t u Vu$
\end{proposition}

Le but de la méthode est d'obtenir une représentation approché du nuage des $n$ individus dans un s.e.v. de dimension faible ceci s'effectue par projection. Il faut déformer le moins possible les distances en projection ce qui signifie que l'inertie du nuage projeté sur le s.e.v. $F_k$ soit maximale. \\

Soit $P$ : la projection M\_{}orthogonale sur le s.e.v. $F_k$. \\
$Pe_i = f_i$ \\
$P^2 = P et \ ^t PM = MP$ \\

\begin{center}
\includegraphics[width=0.7\textwidth]{schémas/projection2.png}
\end{center}

Le nuage projeté est associé au tableau : $X^t P$ \\
car $f_i = P \cdot e_i \quad (\text{vecteur colonne}) \Rightarrow \ ^t f_i = \ ^t e_i \cdot \ ^t P \quad (\text{vecteur ligne})$. \\
On détermine la matrice de variance-covariance du tableau $X \ ^t P$ : \\
$
\begin{aligned}
\ ^t (X \ ^t P) D (X \ ^t P) \quad (\text{les var sont centrées}) &= P \ ^t XDX \ ^t P \\
&= \boxed{P V \ ^t P}
\end{aligned}
$ \\

On détermine l'inertie du nuage projeté : $Trace (PV \ ^t PM)$. \\

$
\begin{aligned}
Tr(PV \ ^t PM) &= Tr(PVMP) \\
&= Tr(VMP^2) \quad (\text{car } Tr(AB) = Tr(BA)) \\
&= Tr(VMP)
\end{aligned}
$ \\

Donc l'inertie du nuage projeté est $Trace(VMP)$. \\
Le problème est donc de trouver $P$ : projection M\_{}orthogonale de rang $k$ maximisant la trace de $VMP$, ce qui déterminera $F_k$ (dim $F_k = k$).

\begin{theoreme}{Théorème}
Soit $F_k$ un s.e.v. portant l'inertie maximale, alors le s.e.v. de dimension $k + 1$ portant l'inertie maximale est la somme directe de $F_k$ et du s.e.v. de dimension 1 M\_{}orthogonal à $F_k$ portant l'inertie maximale. \\
\begin{center}
$\boxed{F_{k + 1} = F_k \oplus b \mathbb{R}} \quad (b\mathbb{R} \text{ est de dimention 1})$
\end{center}
\end{theoreme}

Pour obtenir $F_k$ on pourra procéder de proche en proche en cherchant d'abord le s.e.v. de dimension 1 d'inertie maximal puis le s.e.v. de dimension 1 M\_{}orthogonal au premier d'inertie maximale. On cherche la droite de $\mathbb{R}^P$ passant par $g$, maximisant l'inertie du nuage projeté sur cette droite on rappelle la projection M\_{}orthogonale sur la droite dirigée par $a$: \\
$P = a (\ ^t a Ma)^{-1} \ ^t a M$. \\

Inertie du nuage projeté sur cette droite : \\
$
\begin{aligned}
Tr(VMP) &= Tr(VM a (\ ^t a Ma)^{-1} \ ^t a M) \\
&= \frac{1}{\ ^t a Ma} Tr(VMa \ ^t aM) \\
&= \frac{1}{\ ^t a Ma} Tr(\ ^t a MVMa) \\
&= \frac{\ ^t a MVMa}{\ ^t a Ma}
\end{aligned}
$ \\

$\frac{d}{da} (\frac{\ ^t a MVMa}{\ ^t a Ma}) = 0 \quad (*)$ \\

\begin{rappel}{Rappel}
$\frac{d}{da} (\ ^t a Aa) = Aa + \ ^t A a \quad (\ ^t a Aa \rightarrow \text{forme quadratique})$ \\
si $A$ est symétrique : $\boxed{\frac{d}{da} (\ ^t a Aa) = 2 Aa}$ \\
$
\begin{aligned}
(*) &\Rightarrow \frac{(\ ^t a Ma) 2 MVMa - (\ ^t a MVMa) 2 Ma}{(\ ^t a Ma)^2} = 0 \\
&\Rightarrow MVMa = (\frac{\ ^t a MVMa}{\ ^t aMa}) Ma \\
&\Rightarrow VMa = (\frac{\ ^t a MVMa}{\ ^t a Ma}) a \\
&\Rightarrow \boxed{VMa = \lambda a} \quad (\text{avec } \lambda = \frac{\ ^t a MVMa}{\ ^t a Ma})
\end{aligned}
$ \\
\end{rappel}

Donc $a$ est un vecteur propre de $VM$ associé à $\lambda$ (valeur propre), il faut aus $\lambda$ soit maximale.\\
Donc le s.e.v. $F_k$ de dimension $k$ est engendré par les $k$ vecteurs propres de $VM$ associé aux $k$ plus grandes valeurs propres. \\
On appelle composante principale : $\boxed{C^{(i)} = Yu^{(i)}} \quad u^{(i)} : \text{facteur}$ \\

Si les variables initiales sont centrés alors $C^{(i)} = Xu^{(i)}$ et $\boxed{V(C^{(i)}) = \lambda_i} \quad \forall i$


\subsection{Qualités des représentations sur les plans principaux}

Le but de L'A.C.P. (Analyse Composante Principale) étant d'obtenir une représentation des individus dans un espace de dimension plus faible que $P$. Le critère le plus utilisé est celui du pourcentage d'inertie totale expliquée. On mesure la qualité de $F_k$ par : \\
\begin{center}
\LARGE
$\frac{\lambda_1 + \lambda_2 + \cdots + \lambda_k}{\lambda_1 + \lambda_2 + \cdots + \lambda_p}$
\end{center}

\begin{flushleft}
Inertie totale: $\lambda_1 + \lambda_2 + \cdots + \lambda_p = T_{tot}$ \\
Si par exemple: $\frac{\lambda_1 + \lambda_2}{T_{tot}} = 90\%$ \\
On conçoit qu'une représentation du nuage dans le plan des 2 premiers axes principaux sera très satisfaisante.
\end{flushleft}


\subsection{Corrélation entre composantes principales et variables initiales}
\begin{flushleft}
La méthode la plus naturelle pour donner une signification à une composante principale $C^{(i)}$ est de la relier aux variables principales $X^{(j)}$ (variables initiales) en calculant les coefficients de corrélation linéaire.

\begin{center}
\large
$\rho (X^{(j)} , C^{(i)} )$
\end{center}

Et en s'intéressant aux plus forts coefficients en valeur absolue : \\
\begin{center}
$\boxed{\rho (X^{(j)}, C^{(i)}) = \frac{Cov (X^{(j)}, C^{(i)})}{\sigma (X^{(j)} \sigma (C^{(i)}))}}$
\end{center}

$Cov (X^{(j)}, C^{(i)}) = \langle Y^{(j)} | C^{(i)} \rangle$ \\

Où $Y^{(j)} : $ variable centrée
\end{flushleft}


\subsection{Exercice 7 - A.C.P.}
\begin{flushleft}
Soit $X = 
\begin{pmatrix}
16 & 2 & 0 \\
8 & 12 & 10 \\
12 & 16 & 14 \\
20 & 8 & 14 \\
16 & 4 & 10 \\
0 & 6 & 12 \\
\end{pmatrix}
$ \\

On donne le même poids à tous les individues : $p_i = \frac{1}{6} \quad \forall i$ et $M = I_3$.
\end{flushleft}


\subsubsection{Calculer la moyenne des variables et le centre de gravité}

$p_i = \frac{1}{6} \quad \forall i = 1, 2, 4, 5, 6$ et $M = I_3$ métrique poids de chaque individu. \\

\begin{flushleft}
\textbf{La moyenne des variables} \\
$
\begin{aligned}
X^{(j)} &= \overset{6}{\underset{i = 1}{\sum}} P_i X_i^{(j)} = \frac{1}{6} \overset{6}{\underset{i = 1}{\sum}} x_i^{(j)} \\
\Rightarrow X^{(1)} &= \overset{6}{\underset{i = 1}{\sum}} P_i X_i^{(1)} = \frac{1}{6} \overset{6}{\underset{i = 1}{\sum}} x_i^{(i)} = \frac{72}{6} = 12 \\
\Rightarrow X^{(2)} &= \frac{1}{6} \overset{6}{\underset{i = 1}{\sum}} x_i^{(2)} = \frac{1}{6} 48 = 8 \\
\Rightarrow X^{(3)} &= \frac{1}{6} \overset{6}{\underset{i = 1}{\sum}} x_i^{(3)} = \frac{60}{6} = 10 \\
\end{aligned}
$ \\

Donc $X^{(1)} = 12, X^{(2)} = 8, X^{(3)} = 10$. \\
Le centre de gravité du nuage formé par les 3 individu : $tg = (12, 8, 10)$
\end{flushleft}


\subsubsection{Donner la matrice $Y$}
\begin{center}
$Y = 
\begin{pmatrix}
4 & -6 & -10 \\
-4 & 4 & 0 \\
0 & 8 & 4 \\
8 & 0 & 4 \\
4 & -4 & 0 \\
-12 & -2 & 2 \\
\end{pmatrix}
$ \\
\end{center}


\subsubsection{Calculer la matrice de variance-covariance $V$}
$V = \ ^t YDY$ avec $D = \frac{1}{6} T_6$ \\
\begin{center}
{\Large
$\Rightarrow V = \frac{1^t}{6} YY = 
\begin{pmatrix}
\frac{128}{3} & \frac{-16}{3} & \frac{-16}{3} \\
\frac{-16}{3} & \frac{68}{3} & \frac{44}{3} \\
\frac{-16}{3} & \frac{44}{3} & \frac{68}{3} \\
\end{pmatrix}
$} \\
\end{center}


\subsubsection{Diagonaliser $MV = V$}
\begin{flushleft}
$M = T_3$ : Métrique dans l'espace des individus. \\
$P_V(\lambda) = det(V - \lambda I_3)$ : polynôme caractéristique de $V$. \\

\begin{center}
{\Large
$P_V(\lambda) =
\begin{pmatrix}
\frac{128}{3} - \lambda & \frac{-16}{3} & \frac{-16}{3} \\
\frac{-16}{3} & \frac{68}{3} - \lambda & \frac{44}{3} \\
\frac{-16}{3} & \frac{44}{3} & \frac{68}{3} - \lambda \\
\end{pmatrix}
$} \\
\end{center}

$C_1 \rightarrow C_1 + C_2 + C_3$ \\

\begin{center}
{\Large
$P_V(X) = (32 - \lambda)
\begin{pmatrix}
1 & \frac{-16}{3} & \frac{-16}{3} \\
1 & \frac{68}{3} - \lambda & \frac{44}{3} \\
1 & \frac{44}{3} & \frac{68}{3} - \lambda \\
\end{pmatrix}
$}
\end{center}

Par linéarité \\
$L_2 \rightarrow L_2 - L_1$ et $L_3 \rightarrow L_3 - L_1$

\begin{center}
{\Large
$P_V(X) = (32 - \lambda)
\begin{pmatrix}
1 & \frac{-16}{3} & \frac{-16}{3} \\
0 & 28 - \lambda & 20 \\
0 & 20 & 28 - \lambda \\
\end{pmatrix}
$}
\end{center}

$
\begin{aligned}
P_V(X) &= (32 - \lambda)((28 - \lambda)^2 - 20^2) \\
&= (32 - \lambda)(28 - \lambda - 20)(28 - \lambda + 20) \\
&= (32 - \lambda)(8 - \lambda)(48 - \lambda)
\end{aligned}
$ \\
\medskip
Les valeurs propres de $V : \lambda_1 = 48, \lambda_2 = 32, \lambda_3 = 8$ (dans le sens décroissant).
\end{flushleft}


\subsubsection{Calculer le pourcentage d'inertie}
\begin{flushleft}
Le 1\up{er} axe : $\frac{\lambda_1}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{48}{88} = 0,54$ \\
Le 2\up{ème} axe : $\frac{\lambda_2}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{32}{88} = 0,36$ \\
Le 3\up{ème} axe : $\frac{\lambda_3}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{8}{88} = 0,09$ \\
\end{flushleft}

\begin{flushleft}
L'inertie portée par le plan principal
\end{flushleft}
\begin{center}
$\frac{\lambda_1 + \lambda_2}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{48 + 32}{88} = 90\%$
\end{center}

\begin{flushleft}
L'inertie est maximale. \\

On retiendra que 2 axes principaux (facteur principaux). Ici facteur principaux $\equiv$ axes principaux car $M = I_3 (u = a)$.
\end{flushleft}


\subsubsection{Facteur principaux}
Les facteurs principaux sont les 2 vecteurs propres associés aux valeurs propres $\lambda_1 = 48$ et $\lambda_2 = 32$.

\begin{center}
$E_{48} = Ker(V - 48I_3)$ \\

$\forall u =
\begin{pmatrix}
X \\
Y \\
Z \\
\end{pmatrix}
\in E_{48}$
\end{center}

\begin{center}
$ \Leftrightarrow (V - 48I_3)
\begin{pmatrix}
X \\
Y \\
Z \\
\end{pmatrix}
= 0$
\end{center}

\begin{center}
$
\Leftrightarrow
\left \{
\begin{aligned}
- \frac{16}{3}x - \frac{16}{3}y - \frac{16}{3}z &= 0 \\
- \frac{16}{3}x - \frac{76}{3}y - \frac{44}{3}z &= 0 \\
- \frac{16}{3}x + \frac{66}{3}y - \frac{76}{3}z &= 0 \\
\end{aligned}
\right .
$
\end{center}

\begin{center}
$
\forall v \in Ker(V - 48I_3) \Leftrightarrow
\left \{
\begin{aligned}
x + y + z &= 0 \\
-16x - 76y + 44z &= 0 \\
-16x + 44y - 76z &= 0 \\
\end{aligned}
\right .
$
\end{center}

\begin{center}
$
\begin{aligned}
2 + 3 & \Rightarrow -12y + 12z = 0 \\
& \Rightarrow \boxed{y = z} \\
1 & \Rightarrow \boxed{x = -2y} \\
\end{aligned}
$ \\
\end{center}

\begin{flushleft}
Donc $E_{48} = Vect
\begin{pmatrix}
2 \\
-1 \\
-1 \\
\end{pmatrix}
$
\end{flushleft}

\begin{flushleft}
Le 1\up{er} facteur principal est
\end{flushleft}
\begin{center}
$
u^{(1)} = \frac{1}{\sqrt{6}}
\begin{pmatrix}
2 \\
-1 \\
-1 \\
\end{pmatrix}
$
\end{center}

\begin{flushleft}
Où $\sqrt{6} = $ la norme du vecteur
$
\begin{pmatrix}
2 \\
-1 \\
-1 \\
\end{pmatrix}
$
\end{flushleft}

\begin{center}
$E_{32} = Ker(V - 32I_3)$
\end{center}

\begin{center}
$\forall u = 
\begin{pmatrix}
X \\
Y \\
Z \\
\end{pmatrix}
\in E_ {32}
$
\end{center}

\begin{center}
$\Leftrightarrow (V - 32I_3)
\begin{pmatrix}
X \\
Y \\
Z \\
\end{pmatrix}
 = 0
$
\end{center}

\begin{center}
$
\Leftrightarrow
\left \{
\begin{aligned}
32x - 16y - 16z &= 0 \\
-16x - 28y - 44z &= 0 \\
-16x + 44y - 28z &= 0 \\
\end{aligned}
\right .
$
\end{center}

\begin{center}
$
\Leftrightarrow
\left \{
\begin{aligned}
2x - y - z &= 0 \\
-4x - 7y - 11z &= 0 \\
-4x + 11y - 72 &= 0 \\
\end{aligned}
\right .
$
\end{center}

\begin{center}
$2 - 3 \Rightarrow - 18y + 18z = 0 \Rightarrow \boxed{y = 7}$ \\
$1 \Rightarrow \boxed{x = y}$
\end{center}

\begin{center}
$
E_{32} = Vect
\begin{pmatrix}
1 \\
1 \\
1 \\
\end{pmatrix}
$
\end{center}

\begin{flushleft}
Le 2\up{ème} facteur principal est
\end{flushleft}

\begin{center}
$
u^{(2)} = \frac{1}{\sqrt{3}}
\begin{pmatrix}
1 \\
1 \\
1 \\
\end{pmatrix}
$
\end{center}

\begin{flushleft}
$(u^{(1)}, u^{(2)})$ est une base orthonormée.
\end{flushleft}


\subsubsection{Déterminer les composantes principales et calculer les coefficients de corrélation}









\subsection{Projection}


\end{document}
